{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CGAN_fashion_mnist.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "The **conditional generative adversarial network**(CGAN) is a type of GAN that involves the conditional generation of images by a generator model.\n",
        "And this condition is based on the class label.The architecture of the conditional GAN is composed of generator and discriminator model.\n",
        "As mentioned before that the generator is responsible for generating fake images that are very close to original image to deceive discriminator .The target of the discriminator is to distinguish between real and fake images.\n",
        "The model is trained in adversarial manner . In other words , if the discriminator model shows improvement while dicriminating between real and fake images , this will affect negatively the cost of the generator.\n",
        "\n",
        "The main targets from using the class labels are to improve the GAN(stable,faster training and better quality images) and targeted image generation.\n",
        "\n",
        "There are some limitations applied on a GAN model as it may generate a random images from the domain and also there is a relationship between points in the latent space and the images generated by the generator.\n",
        "\n",
        "Conditional GAN is  trained in  a way that both the generator and the discriminator models are conditioned by the class label which means that when the trained generator model is used as a standalone model to generate images in the domain,  and these images are of a given type, or class label, can be generated."
      ],
      "metadata": {
        "id": "eTM88L-lcsXg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zh1be4cPP18w"
      },
      "outputs": [],
      "source": [
        "from numpy import expand_dims\n",
        "from numpy import zeros\n",
        "from numpy import ones\n",
        "from numpy.random import randn\n",
        "from numpy.random import randint\n",
        "from keras.datasets.fashion_mnist import load_data\n",
        "from tensorflow.keras.optimizers import Adam # - Works\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Reshape\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import Conv2DTranspose\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Concatenate\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The discriminator model\n",
        "\n",
        "An input is defined that takes an integer for the class label of the image to make the image conditional on the provided class label.After that this class lable is passed through embedding layer of size 50(50 classes maps into 50-elements)which will be learnt by the dicriminator model.And the output after that is passed to a FC layer with activation.after that the activations are reshaped into 28×28 activation map which is concatenated with the imput image\n"
      ],
      "metadata": {
        "id": "wapXno1GtivS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # The same as the previous methods but the only different thing is adding labels to the images by concatenate method\n",
        "\n",
        "def MakeDiscriminator(inshape=(28,28,1), n_classes=10):\n",
        "\tinputlabel = Input(shape=(1,))\n",
        "\tli = Embedding(n_classes, 50)(inputlabel)\n",
        "\t\n",
        "\tnodes = inshape[0] * inshape[1]     #scaling image dimensions using linear activation\n",
        "\tli = Dense(nodes)(li)\n",
        " \n",
        "\tli = Reshape((inshape[0], inshape[1], 1))(li)\n",
        "\n",
        "\tinputimage = Input(shape=inshape)\n",
        "\t\n",
        "\tmerge = Concatenate()([inputimage, li])        #the activations are reshaped into 28×28 activation map which is concatenated with the imput image\n",
        "\t\n",
        "\n",
        "\tfeaturemap = Conv2D(128, (3,3), strides=(2,2), padding='same')(merge)    #downsampling\n",
        "\tfeaturemap = LeakyReLU(alpha=0.2)(featuremap)                                    #LeakyRelu activation function\n",
        "\t\n",
        "\tfeaturemap = Conv2D(128, (3,3), strides=(2,2), padding='same')(featuremap)       #downsampling ,128 number of filters,(3,3)kernel,strides(2,2)that controls width and height\n",
        "\t\n",
        "\tfeaturemap = LeakyReLU(alpha=0.2)(featuremap)                                   \n",
        "\t\n",
        "\tfeaturemap = Flatten()(featuremap)                              # Flatten class: flattens the input but does not affect the batch size.\n",
        "                                                                  #convert convolutional 2D layer into 1D of fully connected convolutional network\n",
        "\n",
        "\t\n",
        "\tfeaturemap = Dropout(0.4)(featuremap)                          #The Dropout layer randomly sets input units to 0 with a frequency of rate \n",
        "                                                                 #at each step during training time, which helps prevent overfitting\n",
        "                                                                 #rate represents a Float between 0 and 1. Fraction of the input units to drop, in our\n",
        "                                                                 #code it is 0.4\n",
        " \n",
        "\toutput_layer = Dense(1, activation='sigmoid')(featuremap)\n",
        "\t\n",
        "\n",
        "\tmodel = Model([inputimage, inputlabel], output_layer)\n",
        "\t\n",
        "\topt = Adam(lr=0.0002, beta_1=0.5)\n",
        "\tmodel.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        " \n",
        "\treturn model"
      ],
      "metadata": {
        "id": "7gYGSTinrhM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The use of latent space conditional on the class label is to keep the generator model updated to take class label\n",
        "\n",
        "the same as the discriminator this class lable is passed through embedding layer of size 50(50 classes maps into 50-elements)which will be learnt by the generator model.After that it will be passes to FC(fully connected) layer with linear activation.Then these activations are resized into  single 7×7 feature map which is added to 128 feature map to become 129 feature map that are unsampled.\n",
        "\n"
      ],
      "metadata": {
        "id": "IGpyVZa203wf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Making Generator model"
      ],
      "metadata": {
        "id": "3Na_3nOABcF9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The same as the previous methods but the only different thing is adding labels to the images by concatenate method\n",
        "\n",
        "def MakeGenerator(latent_dim, n_classes=10):\n",
        "\t\n",
        "\tinput_label = Input(shape=(1,))\n",
        "\t\n",
        "\tli = Embedding(n_classes, 50)(input_label)\n",
        "\t\n",
        "\tnodes = 7 * 7\n",
        "\tli = Dense(nodes)(li)\n",
        "\t\n",
        "\tli = Reshape((7, 7, 1))(li)    #reshaping\n",
        "\n",
        "\tin_latent = Input(shape=(latent_dim,))\n",
        "\t\n",
        "\tnodes = 128 * 7 * 7\n",
        "\timage_gen = Dense(nodes)(in_latent)\n",
        "\timage_gen = LeakyReLU(alpha=0.2)(image_gen)\n",
        "\timage_gen = Reshape((7, 7, 128))(image_gen)\n",
        "\t\n",
        "\tmerge = Concatenate()([image_gen, li])    #merging image with input label\n",
        "\t\n",
        "\timage_gen = Conv2DTranspose(128, (4,4), strides=(2,2), padding='same')(merge)    #upsampling to 14*14\n",
        "\timage_gen = LeakyReLU(alpha=0.2)(image_gen)\n",
        "\t\n",
        "\timage_gen = Conv2DTranspose(128, (4,4), strides=(2,2), padding='same')(image_gen)    # upsample to 28x28\n",
        "\timage_gen = LeakyReLU(alpha=0.2)(image_gen)\n",
        "\t\n",
        "\tout_layer = Conv2D(1, (7,7), activation='tanh', padding='same')(image_gen)          # 1 represents filters ,(7,7)represents strides\n",
        "\t\n",
        "\tmodel = Model([in_latent, input_label], out_layer)\n",
        "\treturn model"
      ],
      "metadata": {
        "id": "A8GtAeDV9gkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conditional GAN Making"
      ],
      "metadata": {
        "id": "bKgAQpvuBRWo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The GAN model takes an input from a point in latent space and makes a prediction(real or fake)\n",
        "\n",
        "The discriminator model will take an input (images generated from the generator and class label)\n"
      ],
      "metadata": {
        "id": "K8jt8imtANHt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # we trained the conditional GAN by compiling the generator and discriminator model \n",
        " \n",
        "def MakeGAN(generator_model, discriminator_model):     #the GAN model(generator and discriminator)\n",
        "\t\n",
        "\tdiscriminator_model.trainable = False                 #weights are not trained in the discriminator model\n",
        "\t\n",
        "\tgenerator_noise, generator_label = generator_model.input    #input are taken from generator model(noise and labels)\n",
        "\t\n",
        "\tgenerator_output = generator_model.output\n",
        "\t\n",
        "\tGANoutput = discriminator_model([generator_output, generator_label])      #generated image and labels are the inputs given to the discriminator\n",
        "\t\n",
        "\tmodel = Model([generator_noise, generator_label], GANoutput)            #output classification\n",
        "\t\n",
        "\topt = Adam(lr=0.0002, beta_1=0.5)\n",
        "\tmodel.compile(loss='binary_crossentropy', optimizer=opt)\n",
        "\treturn model\n",
        " "
      ],
      "metadata": {
        "id": "sB-TeZAKsDGS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading the dataset(fashion_minst)"
      ],
      "metadata": {
        "id": "rbHY1ghCF0n4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def LoadRealSamples():\n",
        "\t\n",
        "\t(X_train, Y_train), (_, _) = load_data()\n",
        "\t\n",
        "\tX = expand_dims(X_train, axis=-1)       #expand dimensions to 3dimension by adding channels\n",
        "\t\n",
        "\tX = X.astype('float32')\n",
        "\t\n",
        "\tX = (X - 127.5) / 127.5\n",
        "\treturn [X,  Y_train]"
      ],
      "metadata": {
        "id": "zmTGxI8qsNIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        " Chooses a batch of samples that must be updated to be used for real class labels from training dataset"
      ],
      "metadata": {
        "id": "4BbqMrAYG9Jg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def GenerateRealSamples(dataset, num_samples):\n",
        "\t\n",
        "\timages, labels = dataset\n",
        "\t\n",
        "\tix = randint(0, images.shape[0], num_samples)       #to select random instances\n",
        "\t\n",
        "\tX, labels = images[ix], labels[ix]\n",
        "                                      \n",
        "\ty = ones((num_samples, 1))   #generating class for labels\n",
        "\treturn [X, labels], y"
      ],
      "metadata": {
        "id": "VY4asmvksXAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function should have all the updates to generate array of randomly choosen integer class labels to go along with the randomly selected points from the latent space\n"
      ],
      "metadata": {
        "id": "IiRYhAhyJBtj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def generate_latent_points(latent_dim, num_samples, n_classes=10):   #input=points from latent space\n",
        "\t\n",
        "\tXinput = randn(latent_dim * num_samples)\n",
        "\t\n",
        "\tZinput = \tXinput.reshape(num_samples, latent_dim)     #reshaping\n",
        "\t\n",
        "\tlabels = randint(0, n_classes, num_samples)        #generation of labels\n",
        "\n",
        "\treturn [Zinput, labels]"
      ],
      "metadata": {
        "id": "ayrPHymnsiyU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function is updated to use the randomly generated class labels as input to the generator model while generating fake images"
      ],
      "metadata": {
        "id": "XEDyYHZIKzNQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def FakeSamplesGeneration(generator, latent_dim, num_samples):      #the generator generates fake samples using the labels class\n",
        "\t\n",
        "\tZinput, labels_input = generate_latent_points(latent_dim, num_samples)       #points generation in latent space\n",
        "\t\n",
        "\timages = generator.predict([Zinput, labels_input])            #output prediction\n",
        "\t\n",
        "\ty = zeros((num_samples, 1))\n",
        "\treturn [images, labels_input], y"
      ],
      "metadata": {
        "id": "YhP4QyBCsrRN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function must be updated to use and retrieve the class labels in the calls to update both the generator model and the discriminator model "
      ],
      "metadata": {
        "id": "GVMdGU1dMOJK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def train(generator_model, discriminator_model, GANmodel, dataset, latent_dim, n_epochs=2, n_batch=128):\n",
        "\tbat_per_epo = int(dataset[0].shape[0] / n_batch)\n",
        "\thalf_batch = int(n_batch / 2)\n",
        "\t\n",
        "\tfor i in range(n_epochs):          #  epochs enumeration\n",
        "\t\t\n",
        "\t\tfor j in range(bat_per_epo):              #batch enumeration over the training set\n",
        "\t\t\t\n",
        "\t\t\t[X_real, labels_real], y_real = GenerateRealSamples(dataset, half_batch)       #select real samples randomly\n",
        "\t\t\t\n",
        "\t\t\tdiscriminator_loss1, _ = discriminator_model.train_on_batch([X_real, labels_real], y_real)       #discriminator model weights are updated\n",
        "\t\t\t\n",
        "\t\t\t[X_fake, labels], y_fake = FakeSamplesGeneration(generator_model, latent_dim, half_batch)    #fake samples generation\n",
        "\t\t\t\n",
        "\t\t\tdiscriminator_loss2, _ = discriminator_model.train_on_batch([X_fake, labels], y_fake)   # weights of discriminator model are updated \n",
        "\t\t\t\n",
        "\t\t\t[Zinput, labels_input] = generate_latent_points(latent_dim, n_batch)     #inputs for the generator are the points from latent space\n",
        "\t\t\t\n",
        "\t\t\ty_gan = ones((n_batch, 1))                                   #inverted labels are created for fake samples\n",
        "\t\t\t\n",
        "\t\t\tgenerator_loss = GANmodel.train_on_batch([Zinput, labels_input], y_gan)    #generator is updated via discriminator errors\n",
        "\t\t\t\n",
        "\t\t\tprint('>%d, %d/%d, d1=%.3f, d2=%.3f g=%.3f' %\n",
        "\t\t\t\t(i+1, j+1, bat_per_epo, discriminator_loss1, discriminator_loss2,generator_loss))\n",
        "\t\n",
        "\tgenerator_model.save('cgan_generator.h5')"
      ],
      "metadata": {
        "id": "jPAQ6sKVs-Dx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "latent_dim = 100   #size of latent space\n",
        "\n",
        "discriminator_model = MakeDiscriminator(inshape=(28,28,1))\n",
        "\n",
        "generator_model = MakeGenerator(latent_dim)\n",
        "\n",
        "GANmodel = MakeGAN(generator_model, discriminator_model)\n",
        "\n",
        "dataset = LoadRealSamples()\n",
        "\n",
        "train(generator_model, discriminator_model, GANmodel, dataset, latent_dim)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i470TkVitNTw",
        "outputId": "c0552018-0289-4954-8b00-0e933c23fd96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">1, 1/468, d1=0.703, d2=0.696 g=0.691\n",
            ">1, 2/468, d1=0.626, d2=0.700 g=0.687\n",
            ">1, 3/468, d1=0.561, d2=0.707 g=0.680\n",
            ">1, 4/468, d1=0.510, d2=0.720 g=0.668\n",
            ">1, 5/468, d1=0.458, d2=0.736 g=0.656\n",
            ">1, 6/468, d1=0.415, d2=0.760 g=0.637\n",
            ">1, 7/468, d1=0.379, d2=0.789 g=0.627\n",
            ">1, 8/468, d1=0.352, d2=0.802 g=0.624\n",
            ">1, 9/468, d1=0.320, d2=0.799 g=0.657\n",
            ">1, 10/468, d1=0.321, d2=0.731 g=0.727\n",
            ">1, 11/468, d1=0.318, d2=0.644 g=0.832\n",
            ">1, 12/468, d1=0.315, d2=0.557 g=0.938\n",
            ">1, 13/468, d1=0.345, d2=0.522 g=0.980\n",
            ">1, 14/468, d1=0.367, d2=0.529 g=0.928\n",
            ">1, 15/468, d1=0.332, d2=0.584 g=0.832\n",
            ">1, 16/468, d1=0.310, d2=0.634 g=0.765\n",
            ">1, 17/468, d1=0.265, d2=0.681 g=0.712\n",
            ">1, 18/468, d1=0.220, d2=0.745 g=0.657\n",
            ">1, 19/468, d1=0.230, d2=0.854 g=0.583\n",
            ">1, 20/468, d1=0.135, d2=0.970 g=0.522\n",
            ">1, 21/468, d1=0.127, d2=1.042 g=0.503\n",
            ">1, 22/468, d1=0.130, d2=1.035 g=0.541\n",
            ">1, 23/468, d1=0.115, d2=0.899 g=0.655\n",
            ">1, 24/468, d1=0.107, d2=0.696 g=0.857\n",
            ">1, 25/468, d1=0.090, d2=0.507 g=1.108\n",
            ">1, 26/468, d1=0.091, d2=0.395 g=1.257\n",
            ">1, 27/468, d1=0.069, d2=0.391 g=1.235\n",
            ">1, 28/468, d1=0.059, d2=0.436 g=1.079\n",
            ">1, 29/468, d1=0.058, d2=0.515 g=0.933\n",
            ">1, 30/468, d1=0.055, d2=0.584 g=0.828\n",
            ">1, 31/468, d1=0.053, d2=0.652 g=0.749\n",
            ">1, 32/468, d1=0.067, d2=0.735 g=0.668\n",
            ">1, 33/468, d1=0.057, d2=0.814 g=0.598\n",
            ">1, 34/468, d1=0.033, d2=0.911 g=0.543\n",
            ">1, 35/468, d1=0.039, d2=1.010 g=0.496\n",
            ">1, 36/468, d1=0.053, d2=1.071 g=0.488\n",
            ">1, 37/468, d1=0.085, d2=1.048 g=0.528\n",
            ">1, 38/468, d1=0.103, d2=0.912 g=0.633\n",
            ">1, 39/468, d1=0.137, d2=0.749 g=0.821\n",
            ">1, 40/468, d1=0.192, d2=0.591 g=0.976\n",
            ">1, 41/468, d1=0.198, d2=0.507 g=1.062\n",
            ">1, 42/468, d1=0.284, d2=0.504 g=1.001\n",
            ">1, 43/468, d1=0.251, d2=0.579 g=0.874\n",
            ">1, 44/468, d1=0.163, d2=0.624 g=0.795\n",
            ">1, 45/468, d1=0.230, d2=0.670 g=0.731\n",
            ">1, 46/468, d1=0.145, d2=0.702 g=0.689\n",
            ">1, 47/468, d1=0.120, d2=0.725 g=0.670\n",
            ">1, 48/468, d1=0.103, d2=0.746 g=0.657\n",
            ">1, 49/468, d1=0.132, d2=0.762 g=0.648\n",
            ">1, 50/468, d1=0.119, d2=0.784 g=0.632\n",
            ">1, 51/468, d1=0.095, d2=0.792 g=0.615\n",
            ">1, 52/468, d1=0.096, d2=0.839 g=0.598\n",
            ">1, 53/468, d1=0.075, d2=0.849 g=0.582\n",
            ">1, 54/468, d1=0.107, d2=0.900 g=0.566\n",
            ">1, 55/468, d1=0.101, d2=0.906 g=0.561\n",
            ">1, 56/468, d1=0.103, d2=0.938 g=0.575\n",
            ">1, 57/468, d1=0.159, d2=0.907 g=0.596\n",
            ">1, 58/468, d1=0.235, d2=0.921 g=0.585\n",
            ">1, 59/468, d1=0.263, d2=0.911 g=0.564\n",
            ">1, 60/468, d1=0.287, d2=0.910 g=0.557\n",
            ">1, 61/468, d1=0.238, d2=0.947 g=0.567\n",
            ">1, 62/468, d1=0.309, d2=0.926 g=0.573\n",
            ">1, 63/468, d1=0.198, d2=0.882 g=0.609\n",
            ">1, 64/468, d1=0.265, d2=0.870 g=0.631\n",
            ">1, 65/468, d1=0.275, d2=0.818 g=0.668\n",
            ">1, 66/468, d1=0.263, d2=0.793 g=0.713\n",
            ">1, 67/468, d1=0.302, d2=0.784 g=0.721\n",
            ">1, 68/468, d1=0.322, d2=0.801 g=0.697\n",
            ">1, 69/468, d1=0.356, d2=0.879 g=0.610\n",
            ">1, 70/468, d1=0.306, d2=1.032 g=0.563\n",
            ">1, 71/468, d1=0.293, d2=1.033 g=0.610\n",
            ">1, 72/468, d1=0.313, d2=0.860 g=0.722\n",
            ">1, 73/468, d1=0.346, d2=0.807 g=0.734\n",
            ">1, 74/468, d1=0.295, d2=1.003 g=0.607\n",
            ">1, 75/468, d1=0.531, d2=1.272 g=0.556\n",
            ">1, 76/468, d1=0.521, d2=1.071 g=0.715\n",
            ">1, 77/468, d1=0.580, d2=0.725 g=0.985\n",
            ">1, 78/468, d1=0.691, d2=0.491 g=1.287\n",
            ">1, 79/468, d1=0.709, d2=0.345 g=1.469\n",
            ">1, 80/468, d1=0.692, d2=0.306 g=1.533\n",
            ">1, 81/468, d1=0.619, d2=0.311 g=1.430\n",
            ">1, 82/468, d1=0.568, d2=0.414 g=1.207\n",
            ">1, 83/468, d1=0.590, d2=0.665 g=0.847\n",
            ">1, 84/468, d1=0.599, d2=1.197 g=0.521\n",
            ">1, 85/468, d1=0.559, d2=1.357 g=0.484\n",
            ">1, 86/468, d1=0.566, d2=1.083 g=0.679\n",
            ">1, 87/468, d1=0.500, d2=0.614 g=1.100\n",
            ">1, 88/468, d1=0.463, d2=0.356 g=1.505\n",
            ">1, 89/468, d1=0.419, d2=0.290 g=1.562\n",
            ">1, 90/468, d1=0.334, d2=0.403 g=1.304\n",
            ">1, 91/468, d1=0.345, d2=0.770 g=0.792\n",
            ">1, 92/468, d1=0.313, d2=1.133 g=0.541\n",
            ">1, 93/468, d1=0.406, d2=1.271 g=0.558\n",
            ">1, 94/468, d1=0.571, d2=1.037 g=0.710\n",
            ">1, 95/468, d1=0.611, d2=0.738 g=0.915\n",
            ">1, 96/468, d1=0.751, d2=0.569 g=1.112\n",
            ">1, 97/468, d1=0.827, d2=0.441 g=1.202\n",
            ">1, 98/468, d1=0.765, d2=0.422 g=1.258\n",
            ">1, 99/468, d1=0.788, d2=0.415 g=1.215\n",
            ">1, 100/468, d1=0.728, d2=0.455 g=1.178\n",
            ">1, 101/468, d1=0.684, d2=0.498 g=1.062\n",
            ">1, 102/468, d1=0.681, d2=0.573 g=0.910\n",
            ">1, 103/468, d1=0.635, d2=0.676 g=0.846\n",
            ">1, 104/468, d1=0.664, d2=0.746 g=0.775\n",
            ">1, 105/468, d1=0.604, d2=0.793 g=0.716\n",
            ">1, 106/468, d1=0.612, d2=0.838 g=0.677\n",
            ">1, 107/468, d1=0.695, d2=0.808 g=0.713\n",
            ">1, 108/468, d1=0.606, d2=0.753 g=0.780\n",
            ">1, 109/468, d1=0.613, d2=0.643 g=0.897\n",
            ">1, 110/468, d1=0.594, d2=0.556 g=1.019\n",
            ">1, 111/468, d1=0.543, d2=0.464 g=1.141\n",
            ">1, 112/468, d1=0.505, d2=0.412 g=1.229\n",
            ">1, 113/468, d1=0.440, d2=0.405 g=1.272\n",
            ">1, 114/468, d1=0.398, d2=0.443 g=1.165\n",
            ">1, 115/468, d1=0.398, d2=0.550 g=1.032\n",
            ">1, 116/468, d1=0.369, d2=0.781 g=0.805\n",
            ">1, 117/468, d1=0.367, d2=0.974 g=0.739\n",
            ">1, 118/468, d1=0.467, d2=0.962 g=0.743\n",
            ">1, 119/468, d1=0.567, d2=0.857 g=0.866\n",
            ">1, 120/468, d1=0.644, d2=0.610 g=1.031\n",
            ">1, 121/468, d1=0.706, d2=0.498 g=1.180\n",
            ">1, 122/468, d1=0.752, d2=0.424 g=1.297\n",
            ">1, 123/468, d1=0.708, d2=0.359 g=1.367\n",
            ">1, 124/468, d1=0.734, d2=0.337 g=1.407\n",
            ">1, 125/468, d1=0.622, d2=0.324 g=1.462\n",
            ">1, 126/468, d1=0.695, d2=0.327 g=1.433\n",
            ">1, 127/468, d1=0.476, d2=0.344 g=1.451\n",
            ">1, 128/468, d1=0.578, d2=0.333 g=1.451\n",
            ">1, 129/468, d1=0.459, d2=0.364 g=1.365\n",
            ">1, 130/468, d1=0.522, d2=0.427 g=1.236\n",
            ">1, 131/468, d1=0.536, d2=0.545 g=1.056\n",
            ">1, 132/468, d1=0.455, d2=0.690 g=0.930\n",
            ">1, 133/468, d1=0.606, d2=0.821 g=0.778\n",
            ">1, 134/468, d1=0.744, d2=1.027 g=0.677\n",
            ">1, 135/468, d1=0.651, d2=1.073 g=0.584\n",
            ">1, 136/468, d1=0.609, d2=1.079 g=0.636\n",
            ">1, 137/468, d1=0.618, d2=0.964 g=0.725\n",
            ">1, 138/468, d1=0.639, d2=0.877 g=0.853\n",
            ">1, 139/468, d1=0.575, d2=0.712 g=1.029\n",
            ">1, 140/468, d1=0.520, d2=0.536 g=1.290\n",
            ">1, 141/468, d1=0.494, d2=0.360 g=1.636\n",
            ">1, 142/468, d1=0.445, d2=0.254 g=1.916\n",
            ">1, 143/468, d1=0.409, d2=0.223 g=1.966\n",
            ">1, 144/468, d1=0.366, d2=0.294 g=1.702\n",
            ">1, 145/468, d1=0.368, d2=0.481 g=1.231\n",
            ">1, 146/468, d1=0.260, d2=0.761 g=0.936\n",
            ">1, 147/468, d1=0.368, d2=0.960 g=0.734\n",
            ">1, 148/468, d1=0.442, d2=1.017 g=0.725\n",
            ">1, 149/468, d1=0.455, d2=0.874 g=0.779\n",
            ">1, 150/468, d1=0.686, d2=0.790 g=0.812\n",
            ">1, 151/468, d1=0.679, d2=0.707 g=0.897\n",
            ">1, 152/468, d1=0.771, d2=0.634 g=0.956\n",
            ">1, 153/468, d1=0.675, d2=0.574 g=1.059\n",
            ">1, 154/468, d1=0.704, d2=0.492 g=1.144\n",
            ">1, 155/468, d1=0.722, d2=0.428 g=1.301\n",
            ">1, 156/468, d1=0.689, d2=0.369 g=1.351\n",
            ">1, 157/468, d1=0.680, d2=0.321 g=1.422\n",
            ">1, 158/468, d1=0.624, d2=0.314 g=1.492\n",
            ">1, 159/468, d1=0.590, d2=0.281 g=1.591\n",
            ">1, 160/468, d1=0.579, d2=0.260 g=1.602\n",
            ">1, 161/468, d1=0.479, d2=0.252 g=1.676\n",
            ">1, 162/468, d1=0.562, d2=0.259 g=1.584\n",
            ">1, 163/468, d1=0.495, d2=0.258 g=1.513\n",
            ">1, 164/468, d1=0.482, d2=0.329 g=1.425\n",
            ">1, 165/468, d1=0.448, d2=0.394 g=1.332\n",
            ">1, 166/468, d1=0.448, d2=0.497 g=1.213\n",
            ">1, 167/468, d1=0.478, d2=0.614 g=0.927\n",
            ">1, 168/468, d1=0.534, d2=0.873 g=0.745\n",
            ">1, 169/468, d1=0.549, d2=1.090 g=0.570\n",
            ">1, 170/468, d1=0.679, d2=1.491 g=0.392\n",
            ">1, 171/468, d1=0.793, d2=1.561 g=0.376\n",
            ">1, 172/468, d1=0.733, d2=1.463 g=0.386\n",
            ">1, 173/468, d1=0.834, d2=1.210 g=0.454\n",
            ">1, 174/468, d1=0.786, d2=1.047 g=0.601\n",
            ">1, 175/468, d1=0.791, d2=0.864 g=0.761\n",
            ">1, 176/468, d1=0.729, d2=0.655 g=0.879\n",
            ">1, 177/468, d1=0.717, d2=0.542 g=0.991\n",
            ">1, 178/468, d1=0.703, d2=0.496 g=1.069\n",
            ">1, 179/468, d1=0.694, d2=0.502 g=1.094\n",
            ">1, 180/468, d1=0.659, d2=0.493 g=1.026\n",
            ">1, 181/468, d1=0.637, d2=0.568 g=0.963\n",
            ">1, 182/468, d1=0.653, d2=0.601 g=0.879\n",
            ">1, 183/468, d1=0.643, d2=0.701 g=0.758\n",
            ">1, 184/468, d1=0.587, d2=0.749 g=0.733\n",
            ">1, 185/468, d1=0.618, d2=0.788 g=0.686\n",
            ">1, 186/468, d1=0.630, d2=0.830 g=0.680\n",
            ">1, 187/468, d1=0.666, d2=0.793 g=0.690\n",
            ">1, 188/468, d1=0.646, d2=0.763 g=0.691\n",
            ">1, 189/468, d1=0.647, d2=0.759 g=0.710\n",
            ">1, 190/468, d1=0.632, d2=0.712 g=0.744\n",
            ">1, 191/468, d1=0.657, d2=0.687 g=0.774\n",
            ">1, 192/468, d1=0.697, d2=0.648 g=0.781\n",
            ">1, 193/468, d1=0.669, d2=0.664 g=0.805\n",
            ">1, 194/468, d1=0.671, d2=0.647 g=0.824\n",
            ">1, 195/468, d1=0.693, d2=0.631 g=0.817\n",
            ">1, 196/468, d1=0.677, d2=0.641 g=0.807\n",
            ">1, 197/468, d1=0.617, d2=0.649 g=0.817\n",
            ">1, 198/468, d1=0.642, d2=0.634 g=0.810\n",
            ">1, 199/468, d1=0.650, d2=0.647 g=0.793\n",
            ">1, 200/468, d1=0.635, d2=0.663 g=0.782\n",
            ">1, 201/468, d1=0.643, d2=0.652 g=0.787\n",
            ">1, 202/468, d1=0.619, d2=0.664 g=0.756\n",
            ">1, 203/468, d1=0.697, d2=0.664 g=0.762\n",
            ">1, 204/468, d1=0.674, d2=0.692 g=0.746\n",
            ">1, 205/468, d1=0.588, d2=0.723 g=0.750\n",
            ">1, 206/468, d1=0.607, d2=0.717 g=0.735\n",
            ">1, 207/468, d1=0.612, d2=0.728 g=0.712\n",
            ">1, 208/468, d1=0.632, d2=0.750 g=0.739\n",
            ">1, 209/468, d1=0.620, d2=0.770 g=0.695\n",
            ">1, 210/468, d1=0.631, d2=0.760 g=0.700\n",
            ">1, 211/468, d1=0.600, d2=0.780 g=0.705\n",
            ">1, 212/468, d1=0.646, d2=0.765 g=0.726\n",
            ">1, 213/468, d1=0.633, d2=0.746 g=0.745\n",
            ">1, 214/468, d1=0.656, d2=0.712 g=0.796\n",
            ">1, 215/468, d1=0.636, d2=0.653 g=0.861\n",
            ">1, 216/468, d1=0.611, d2=0.608 g=0.902\n",
            ">1, 217/468, d1=0.590, d2=0.540 g=0.961\n",
            ">1, 218/468, d1=0.585, d2=0.525 g=1.003\n",
            ">1, 219/468, d1=0.579, d2=0.531 g=1.028\n",
            ">1, 220/468, d1=0.597, d2=0.507 g=1.042\n",
            ">1, 221/468, d1=0.586, d2=0.505 g=1.041\n",
            ">1, 222/468, d1=0.622, d2=0.503 g=0.998\n",
            ">1, 223/468, d1=0.565, d2=0.561 g=0.974\n",
            ">1, 224/468, d1=0.564, d2=0.572 g=0.952\n",
            ">1, 225/468, d1=0.597, d2=0.630 g=0.853\n",
            ">1, 226/468, d1=0.593, d2=0.693 g=0.787\n",
            ">1, 227/468, d1=0.606, d2=0.782 g=0.710\n",
            ">1, 228/468, d1=0.524, d2=0.809 g=0.668\n",
            ">1, 229/468, d1=0.571, d2=0.853 g=0.645\n",
            ">1, 230/468, d1=0.596, d2=0.868 g=0.625\n",
            ">1, 231/468, d1=0.570, d2=0.885 g=0.649\n",
            ">1, 232/468, d1=0.586, d2=0.820 g=0.708\n",
            ">1, 233/468, d1=0.608, d2=0.790 g=0.725\n",
            ">1, 234/468, d1=0.670, d2=0.760 g=0.730\n",
            ">1, 235/468, d1=0.694, d2=0.738 g=0.769\n",
            ">1, 236/468, d1=0.677, d2=0.723 g=0.778\n",
            ">1, 237/468, d1=0.662, d2=0.704 g=0.791\n",
            ">1, 238/468, d1=0.674, d2=0.681 g=0.799\n",
            ">1, 239/468, d1=0.662, d2=0.659 g=0.833\n",
            ">1, 240/468, d1=0.689, d2=0.630 g=0.866\n",
            ">1, 241/468, d1=0.670, d2=0.583 g=0.920\n",
            ">1, 242/468, d1=0.739, d2=0.571 g=0.947\n",
            ">1, 243/468, d1=0.719, d2=0.555 g=0.947\n",
            ">1, 244/468, d1=0.717, d2=0.553 g=0.958\n",
            ">1, 245/468, d1=0.659, d2=0.527 g=0.972\n",
            ">1, 246/468, d1=0.660, d2=0.548 g=0.984\n",
            ">1, 247/468, d1=0.642, d2=0.523 g=1.009\n",
            ">1, 248/468, d1=0.585, d2=0.543 g=0.964\n",
            ">1, 249/468, d1=0.644, d2=0.550 g=0.975\n",
            ">1, 250/468, d1=0.590, d2=0.596 g=0.895\n",
            ">1, 251/468, d1=0.627, d2=0.614 g=0.864\n",
            ">1, 252/468, d1=0.618, d2=0.672 g=0.798\n",
            ">1, 253/468, d1=0.619, d2=0.712 g=0.745\n",
            ">1, 254/468, d1=0.670, d2=0.767 g=0.680\n",
            ">1, 255/468, d1=0.609, d2=0.779 g=0.663\n",
            ">1, 256/468, d1=0.643, d2=0.819 g=0.628\n",
            ">1, 257/468, d1=0.609, d2=0.868 g=0.637\n",
            ">1, 258/468, d1=0.629, d2=0.954 g=0.642\n",
            ">1, 259/468, d1=0.659, d2=0.889 g=0.637\n",
            ">1, 260/468, d1=0.629, d2=0.852 g=0.649\n",
            ">1, 261/468, d1=0.705, d2=0.799 g=0.685\n",
            ">1, 262/468, d1=0.674, d2=0.744 g=0.725\n",
            ">1, 263/468, d1=0.660, d2=0.684 g=0.757\n",
            ">1, 264/468, d1=0.645, d2=0.649 g=0.794\n",
            ">1, 265/468, d1=0.617, d2=0.680 g=0.833\n",
            ">1, 266/468, d1=0.627, d2=0.640 g=0.855\n",
            ">1, 267/468, d1=0.599, d2=0.648 g=0.844\n",
            ">1, 268/468, d1=0.629, d2=0.635 g=0.823\n",
            ">1, 269/468, d1=0.668, d2=0.661 g=0.801\n",
            ">1, 270/468, d1=0.636, d2=0.703 g=0.794\n",
            ">1, 271/468, d1=0.601, d2=0.707 g=0.757\n",
            ">1, 272/468, d1=0.594, d2=0.749 g=0.746\n",
            ">1, 273/468, d1=0.635, d2=0.745 g=0.714\n",
            ">1, 274/468, d1=0.667, d2=0.749 g=0.722\n",
            ">1, 275/468, d1=0.615, d2=0.763 g=0.712\n",
            ">1, 276/468, d1=0.660, d2=0.765 g=0.696\n",
            ">1, 277/468, d1=0.715, d2=0.766 g=0.702\n",
            ">1, 278/468, d1=0.673, d2=0.753 g=0.722\n",
            ">1, 279/468, d1=0.711, d2=0.711 g=0.759\n",
            ">1, 280/468, d1=0.709, d2=0.698 g=0.777\n",
            ">1, 281/468, d1=0.727, d2=0.702 g=0.785\n",
            ">1, 282/468, d1=0.726, d2=0.649 g=0.803\n",
            ">1, 283/468, d1=0.703, d2=0.657 g=0.848\n",
            ">1, 284/468, d1=0.699, d2=0.614 g=0.874\n",
            ">1, 285/468, d1=0.716, d2=0.589 g=0.894\n",
            ">1, 286/468, d1=0.772, d2=0.607 g=0.904\n",
            ">1, 287/468, d1=0.680, d2=0.582 g=0.902\n",
            ">1, 288/468, d1=0.724, d2=0.598 g=0.898\n",
            ">1, 289/468, d1=0.731, d2=0.614 g=0.859\n",
            ">1, 290/468, d1=0.638, d2=0.621 g=0.848\n",
            ">1, 291/468, d1=0.682, d2=0.609 g=0.837\n",
            ">1, 292/468, d1=0.682, d2=0.661 g=0.790\n",
            ">1, 293/468, d1=0.633, d2=0.663 g=0.765\n",
            ">1, 294/468, d1=0.729, d2=0.744 g=0.707\n",
            ">1, 295/468, d1=0.646, d2=0.757 g=0.687\n",
            ">1, 296/468, d1=0.648, d2=0.810 g=0.677\n",
            ">1, 297/468, d1=0.677, d2=0.833 g=0.640\n",
            ">1, 298/468, d1=0.670, d2=0.861 g=0.653\n",
            ">1, 299/468, d1=0.715, d2=0.825 g=0.669\n",
            ">1, 300/468, d1=0.731, d2=0.832 g=0.687\n",
            ">1, 301/468, d1=0.688, d2=0.752 g=0.750\n",
            ">1, 302/468, d1=0.687, d2=0.696 g=0.776\n",
            ">1, 303/468, d1=0.700, d2=0.613 g=0.859\n",
            ">1, 304/468, d1=0.705, d2=0.585 g=0.897\n",
            ">1, 305/468, d1=0.667, d2=0.535 g=1.005\n",
            ">1, 306/468, d1=0.695, d2=0.499 g=1.062\n",
            ">1, 307/468, d1=0.665, d2=0.459 g=1.113\n",
            ">1, 308/468, d1=0.673, d2=0.455 g=1.079\n",
            ">1, 309/468, d1=0.672, d2=0.488 g=1.045\n",
            ">1, 310/468, d1=0.636, d2=0.536 g=0.982\n",
            ">1, 311/468, d1=0.655, d2=0.580 g=0.948\n",
            ">1, 312/468, d1=0.581, d2=0.620 g=0.827\n",
            ">1, 313/468, d1=0.579, d2=0.701 g=0.794\n",
            ">1, 314/468, d1=0.676, d2=0.796 g=0.681\n",
            ">1, 315/468, d1=0.613, d2=0.846 g=0.642\n",
            ">1, 316/468, d1=0.674, d2=0.931 g=0.598\n",
            ">1, 317/468, d1=0.616, d2=0.928 g=0.594\n",
            ">1, 318/468, d1=0.768, d2=0.922 g=0.595\n",
            ">1, 319/468, d1=0.681, d2=0.922 g=0.587\n",
            ">1, 320/468, d1=0.715, d2=0.876 g=0.627\n",
            ">1, 321/468, d1=0.737, d2=0.804 g=0.667\n",
            ">1, 322/468, d1=0.757, d2=0.756 g=0.715\n",
            ">1, 323/468, d1=0.763, d2=0.707 g=0.756\n",
            ">1, 324/468, d1=0.752, d2=0.675 g=0.803\n",
            ">1, 325/468, d1=0.762, d2=0.630 g=0.831\n",
            ">1, 326/468, d1=0.779, d2=0.630 g=0.854\n",
            ">1, 327/468, d1=0.755, d2=0.584 g=0.891\n",
            ">1, 328/468, d1=0.722, d2=0.596 g=0.901\n",
            ">1, 329/468, d1=0.760, d2=0.595 g=0.944\n",
            ">1, 330/468, d1=0.706, d2=0.552 g=0.955\n",
            ">1, 331/468, d1=0.804, d2=0.567 g=0.942\n",
            ">1, 332/468, d1=0.754, d2=0.555 g=0.936\n",
            ">1, 333/468, d1=0.744, d2=0.546 g=0.968\n",
            ">1, 334/468, d1=0.720, d2=0.566 g=0.991\n",
            ">1, 335/468, d1=0.795, d2=0.521 g=0.970\n",
            ">1, 336/468, d1=0.713, d2=0.568 g=0.951\n",
            ">1, 337/468, d1=0.653, d2=0.621 g=0.873\n",
            ">1, 338/468, d1=0.645, d2=0.649 g=0.832\n",
            ">1, 339/468, d1=0.631, d2=0.723 g=0.740\n",
            ">1, 340/468, d1=0.670, d2=0.804 g=0.694\n",
            ">1, 341/468, d1=0.629, d2=0.865 g=0.682\n",
            ">1, 342/468, d1=0.743, d2=0.782 g=0.702\n",
            ">1, 343/468, d1=0.714, d2=0.776 g=0.731\n",
            ">1, 344/468, d1=0.719, d2=0.741 g=0.776\n",
            ">1, 345/468, d1=0.674, d2=0.685 g=0.822\n",
            ">1, 346/468, d1=0.712, d2=0.624 g=0.913\n",
            ">1, 347/468, d1=0.705, d2=0.529 g=1.004\n",
            ">1, 348/468, d1=0.714, d2=0.510 g=1.034\n",
            ">1, 349/468, d1=0.733, d2=0.479 g=1.086\n",
            ">1, 350/468, d1=0.733, d2=0.499 g=1.038\n",
            ">1, 351/468, d1=0.730, d2=0.478 g=0.997\n",
            ">1, 352/468, d1=0.675, d2=0.545 g=0.940\n",
            ">1, 353/468, d1=0.734, d2=0.581 g=0.876\n",
            ">1, 354/468, d1=0.718, d2=0.642 g=0.817\n",
            ">1, 355/468, d1=0.660, d2=0.670 g=0.781\n",
            ">1, 356/468, d1=0.715, d2=0.715 g=0.737\n",
            ">1, 357/468, d1=0.720, d2=0.733 g=0.725\n",
            ">1, 358/468, d1=0.656, d2=0.738 g=0.690\n",
            ">1, 359/468, d1=0.638, d2=0.732 g=0.683\n",
            ">1, 360/468, d1=0.645, d2=0.772 g=0.694\n",
            ">1, 361/468, d1=0.641, d2=0.732 g=0.694\n",
            ">1, 362/468, d1=0.659, d2=0.750 g=0.692\n",
            ">1, 363/468, d1=0.677, d2=0.743 g=0.714\n",
            ">1, 364/468, d1=0.699, d2=0.719 g=0.717\n",
            ">1, 365/468, d1=0.702, d2=0.705 g=0.743\n",
            ">1, 366/468, d1=0.672, d2=0.692 g=0.760\n",
            ">1, 367/468, d1=0.671, d2=0.681 g=0.758\n",
            ">1, 368/468, d1=0.652, d2=0.668 g=0.795\n",
            ">1, 369/468, d1=0.717, d2=0.639 g=0.807\n",
            ">1, 370/468, d1=0.691, d2=0.617 g=0.822\n",
            ">1, 371/468, d1=0.680, d2=0.613 g=0.845\n",
            ">1, 372/468, d1=0.675, d2=0.575 g=0.860\n",
            ">1, 373/468, d1=0.694, d2=0.585 g=0.885\n",
            ">1, 374/468, d1=0.662, d2=0.587 g=0.887\n",
            ">1, 375/468, d1=0.638, d2=0.575 g=0.885\n",
            ">1, 376/468, d1=0.625, d2=0.589 g=0.905\n",
            ">1, 377/468, d1=0.683, d2=0.580 g=0.888\n",
            ">1, 378/468, d1=0.691, d2=0.592 g=0.873\n",
            ">1, 379/468, d1=0.712, d2=0.618 g=0.808\n",
            ">1, 380/468, d1=0.684, d2=0.657 g=0.770\n",
            ">1, 381/468, d1=0.674, d2=0.665 g=0.760\n",
            ">1, 382/468, d1=0.687, d2=0.728 g=0.727\n",
            ">1, 383/468, d1=0.620, d2=0.726 g=0.699\n",
            ">1, 384/468, d1=0.723, d2=0.743 g=0.702\n",
            ">1, 385/468, d1=0.622, d2=0.758 g=0.681\n",
            ">1, 386/468, d1=0.720, d2=0.779 g=0.673\n",
            ">1, 387/468, d1=0.718, d2=0.800 g=0.648\n",
            ">1, 388/468, d1=0.694, d2=0.815 g=0.682\n",
            ">1, 389/468, d1=0.713, d2=0.769 g=0.700\n",
            ">1, 390/468, d1=0.703, d2=0.790 g=0.711\n",
            ">1, 391/468, d1=0.727, d2=0.725 g=0.735\n",
            ">1, 392/468, d1=0.742, d2=0.706 g=0.804\n",
            ">1, 393/468, d1=0.745, d2=0.640 g=0.850\n",
            ">1, 394/468, d1=0.744, d2=0.617 g=0.877\n",
            ">1, 395/468, d1=0.707, d2=0.568 g=0.900\n",
            ">1, 396/468, d1=0.730, d2=0.562 g=0.925\n",
            ">1, 397/468, d1=0.704, d2=0.533 g=1.010\n",
            ">1, 398/468, d1=0.728, d2=0.543 g=0.953\n",
            ">1, 399/468, d1=0.712, d2=0.516 g=0.938\n",
            ">1, 400/468, d1=0.680, d2=0.547 g=0.943\n",
            ">1, 401/468, d1=0.693, d2=0.537 g=0.899\n",
            ">1, 402/468, d1=0.687, d2=0.574 g=0.881\n",
            ">1, 403/468, d1=0.678, d2=0.602 g=0.802\n",
            ">1, 404/468, d1=0.650, d2=0.658 g=0.806\n",
            ">1, 405/468, d1=0.719, d2=0.687 g=0.739\n",
            ">1, 406/468, d1=0.697, d2=0.749 g=0.714\n",
            ">1, 407/468, d1=0.621, d2=0.778 g=0.682\n",
            ">1, 408/468, d1=0.663, d2=0.758 g=0.681\n",
            ">1, 409/468, d1=0.680, d2=0.776 g=0.669\n",
            ">1, 410/468, d1=0.683, d2=0.803 g=0.639\n",
            ">1, 411/468, d1=0.658, d2=0.793 g=0.652\n",
            ">1, 412/468, d1=0.649, d2=0.788 g=0.638\n",
            ">1, 413/468, d1=0.638, d2=0.802 g=0.653\n",
            ">1, 414/468, d1=0.649, d2=0.764 g=0.677\n",
            ">1, 415/468, d1=0.615, d2=0.763 g=0.668\n",
            ">1, 416/468, d1=0.674, d2=0.768 g=0.690\n",
            ">1, 417/468, d1=0.649, d2=0.726 g=0.692\n",
            ">1, 418/468, d1=0.655, d2=0.714 g=0.704\n",
            ">1, 419/468, d1=0.628, d2=0.716 g=0.714\n",
            ">1, 420/468, d1=0.663, d2=0.705 g=0.738\n",
            ">1, 421/468, d1=0.647, d2=0.677 g=0.742\n",
            ">1, 422/468, d1=0.643, d2=0.684 g=0.767\n",
            ">1, 423/468, d1=0.647, d2=0.656 g=0.777\n",
            ">1, 424/468, d1=0.633, d2=0.655 g=0.770\n",
            ">1, 425/468, d1=0.622, d2=0.637 g=0.787\n",
            ">1, 426/468, d1=0.642, d2=0.646 g=0.800\n",
            ">1, 427/468, d1=0.607, d2=0.652 g=0.794\n",
            ">1, 428/468, d1=0.587, d2=0.640 g=0.775\n",
            ">1, 429/468, d1=0.642, d2=0.645 g=0.789\n",
            ">1, 430/468, d1=0.589, d2=0.658 g=0.757\n",
            ">1, 431/468, d1=0.627, d2=0.680 g=0.754\n",
            ">1, 432/468, d1=0.626, d2=0.673 g=0.736\n",
            ">1, 433/468, d1=0.632, d2=0.735 g=0.709\n",
            ">1, 434/468, d1=0.616, d2=0.744 g=0.701\n",
            ">1, 435/468, d1=0.577, d2=0.771 g=0.689\n",
            ">1, 436/468, d1=0.646, d2=0.765 g=0.684\n",
            ">1, 437/468, d1=0.674, d2=0.767 g=0.668\n",
            ">1, 438/468, d1=0.618, d2=0.766 g=0.671\n",
            ">1, 439/468, d1=0.645, d2=0.793 g=0.665\n",
            ">1, 440/468, d1=0.671, d2=0.775 g=0.653\n",
            ">1, 441/468, d1=0.668, d2=0.770 g=0.683\n",
            ">1, 442/468, d1=0.683, d2=0.744 g=0.701\n",
            ">1, 443/468, d1=0.660, d2=0.706 g=0.712\n",
            ">1, 444/468, d1=0.669, d2=0.685 g=0.743\n",
            ">1, 445/468, d1=0.693, d2=0.667 g=0.761\n",
            ">1, 446/468, d1=0.689, d2=0.652 g=0.790\n",
            ">1, 447/468, d1=0.698, d2=0.671 g=0.807\n",
            ">1, 448/468, d1=0.699, d2=0.651 g=0.818\n",
            ">1, 449/468, d1=0.687, d2=0.635 g=0.801\n",
            ">1, 450/468, d1=0.691, d2=0.624 g=0.813\n",
            ">1, 451/468, d1=0.688, d2=0.624 g=0.800\n",
            ">1, 452/468, d1=0.690, d2=0.646 g=0.794\n",
            ">1, 453/468, d1=0.694, d2=0.651 g=0.776\n",
            ">1, 454/468, d1=0.649, d2=0.682 g=0.771\n",
            ">1, 455/468, d1=0.671, d2=0.662 g=0.758\n",
            ">1, 456/468, d1=0.717, d2=0.687 g=0.742\n",
            ">1, 457/468, d1=0.716, d2=0.730 g=0.700\n",
            ">1, 458/468, d1=0.688, d2=0.738 g=0.687\n",
            ">1, 459/468, d1=0.688, d2=0.744 g=0.700\n",
            ">1, 460/468, d1=0.668, d2=0.711 g=0.698\n",
            ">1, 461/468, d1=0.652, d2=0.759 g=0.682\n",
            ">1, 462/468, d1=0.665, d2=0.767 g=0.694\n",
            ">1, 463/468, d1=0.701, d2=0.742 g=0.693\n",
            ">1, 464/468, d1=0.719, d2=0.748 g=0.701\n",
            ">1, 465/468, d1=0.721, d2=0.713 g=0.688\n",
            ">1, 466/468, d1=0.685, d2=0.718 g=0.694\n",
            ">1, 467/468, d1=0.677, d2=0.715 g=0.686\n",
            ">1, 468/468, d1=0.666, d2=0.720 g=0.699\n",
            ">2, 1/468, d1=0.661, d2=0.711 g=0.715\n",
            ">2, 2/468, d1=0.671, d2=0.700 g=0.692\n",
            ">2, 3/468, d1=0.669, d2=0.699 g=0.704\n",
            ">2, 4/468, d1=0.699, d2=0.678 g=0.710\n",
            ">2, 5/468, d1=0.684, d2=0.700 g=0.716\n",
            ">2, 6/468, d1=0.671, d2=0.688 g=0.714\n",
            ">2, 7/468, d1=0.666, d2=0.702 g=0.725\n",
            ">2, 8/468, d1=0.685, d2=0.693 g=0.725\n",
            ">2, 9/468, d1=0.670, d2=0.690 g=0.735\n",
            ">2, 10/468, d1=0.684, d2=0.697 g=0.723\n",
            ">2, 11/468, d1=0.665, d2=0.691 g=0.720\n",
            ">2, 12/468, d1=0.647, d2=0.696 g=0.722\n",
            ">2, 13/468, d1=0.672, d2=0.701 g=0.728\n",
            ">2, 14/468, d1=0.665, d2=0.712 g=0.716\n",
            ">2, 15/468, d1=0.656, d2=0.726 g=0.712\n",
            ">2, 16/468, d1=0.658, d2=0.694 g=0.711\n",
            ">2, 17/468, d1=0.642, d2=0.707 g=0.708\n",
            ">2, 18/468, d1=0.658, d2=0.711 g=0.712\n",
            ">2, 19/468, d1=0.673, d2=0.708 g=0.713\n",
            ">2, 20/468, d1=0.696, d2=0.710 g=0.722\n",
            ">2, 21/468, d1=0.661, d2=0.684 g=0.740\n",
            ">2, 22/468, d1=0.702, d2=0.682 g=0.741\n",
            ">2, 23/468, d1=0.676, d2=0.667 g=0.745\n",
            ">2, 24/468, d1=0.668, d2=0.660 g=0.756\n",
            ">2, 25/468, d1=0.651, d2=0.669 g=0.766\n",
            ">2, 26/468, d1=0.659, d2=0.660 g=0.772\n",
            ">2, 27/468, d1=0.685, d2=0.632 g=0.799\n",
            ">2, 28/468, d1=0.659, d2=0.624 g=0.787\n",
            ">2, 29/468, d1=0.654, d2=0.631 g=0.818\n",
            ">2, 30/468, d1=0.636, d2=0.615 g=0.813\n",
            ">2, 31/468, d1=0.659, d2=0.612 g=0.806\n",
            ">2, 32/468, d1=0.650, d2=0.590 g=0.813\n",
            ">2, 33/468, d1=0.686, d2=0.618 g=0.825\n",
            ">2, 34/468, d1=0.641, d2=0.619 g=0.821\n",
            ">2, 35/468, d1=0.659, d2=0.614 g=0.827\n",
            ">2, 36/468, d1=0.612, d2=0.635 g=0.787\n",
            ">2, 37/468, d1=0.622, d2=0.640 g=0.803\n",
            ">2, 38/468, d1=0.643, d2=0.620 g=0.795\n",
            ">2, 39/468, d1=0.653, d2=0.626 g=0.792\n",
            ">2, 40/468, d1=0.648, d2=0.628 g=0.776\n",
            ">2, 41/468, d1=0.635, d2=0.648 g=0.775\n",
            ">2, 42/468, d1=0.641, d2=0.643 g=0.790\n",
            ">2, 43/468, d1=0.633, d2=0.639 g=0.768\n",
            ">2, 44/468, d1=0.646, d2=0.621 g=0.787\n",
            ">2, 45/468, d1=0.617, d2=0.658 g=0.780\n",
            ">2, 46/468, d1=0.606, d2=0.678 g=0.768\n",
            ">2, 47/468, d1=0.615, d2=0.629 g=0.764\n",
            ">2, 48/468, d1=0.632, d2=0.656 g=0.754\n",
            ">2, 49/468, d1=0.631, d2=0.672 g=0.775\n",
            ">2, 50/468, d1=0.650, d2=0.655 g=0.763\n",
            ">2, 51/468, d1=0.630, d2=0.645 g=0.758\n",
            ">2, 52/468, d1=0.594, d2=0.683 g=0.762\n",
            ">2, 53/468, d1=0.622, d2=0.650 g=0.758\n",
            ">2, 54/468, d1=0.608, d2=0.680 g=0.757\n",
            ">2, 55/468, d1=0.612, d2=0.665 g=0.758\n",
            ">2, 56/468, d1=0.637, d2=0.660 g=0.748\n",
            ">2, 57/468, d1=0.627, d2=0.662 g=0.757\n",
            ">2, 58/468, d1=0.634, d2=0.676 g=0.762\n",
            ">2, 59/468, d1=0.595, d2=0.650 g=0.741\n",
            ">2, 60/468, d1=0.621, d2=0.681 g=0.744\n",
            ">2, 61/468, d1=0.619, d2=0.678 g=0.730\n",
            ">2, 62/468, d1=0.603, d2=0.693 g=0.732\n",
            ">2, 63/468, d1=0.630, d2=0.674 g=0.726\n",
            ">2, 64/468, d1=0.635, d2=0.698 g=0.732\n",
            ">2, 65/468, d1=0.626, d2=0.698 g=0.738\n",
            ">2, 66/468, d1=0.622, d2=0.703 g=0.711\n",
            ">2, 67/468, d1=0.626, d2=0.701 g=0.689\n",
            ">2, 68/468, d1=0.642, d2=0.726 g=0.718\n",
            ">2, 69/468, d1=0.632, d2=0.716 g=0.722\n",
            ">2, 70/468, d1=0.637, d2=0.708 g=0.723\n",
            ">2, 71/468, d1=0.657, d2=0.699 g=0.723\n",
            ">2, 72/468, d1=0.646, d2=0.670 g=0.757\n",
            ">2, 73/468, d1=0.658, d2=0.681 g=0.759\n",
            ">2, 74/468, d1=0.645, d2=0.672 g=0.784\n",
            ">2, 75/468, d1=0.638, d2=0.666 g=0.764\n",
            ">2, 76/468, d1=0.657, d2=0.661 g=0.772\n",
            ">2, 77/468, d1=0.628, d2=0.662 g=0.765\n",
            ">2, 78/468, d1=0.659, d2=0.672 g=0.770\n",
            ">2, 79/468, d1=0.628, d2=0.686 g=0.758\n",
            ">2, 80/468, d1=0.637, d2=0.674 g=0.741\n",
            ">2, 81/468, d1=0.618, d2=0.667 g=0.752\n",
            ">2, 82/468, d1=0.656, d2=0.682 g=0.754\n",
            ">2, 83/468, d1=0.638, d2=0.705 g=0.743\n",
            ">2, 84/468, d1=0.642, d2=0.657 g=0.749\n",
            ">2, 85/468, d1=0.653, d2=0.678 g=0.753\n",
            ">2, 86/468, d1=0.627, d2=0.676 g=0.753\n",
            ">2, 87/468, d1=0.623, d2=0.680 g=0.769\n",
            ">2, 88/468, d1=0.629, d2=0.681 g=0.767\n",
            ">2, 89/468, d1=0.623, d2=0.656 g=0.775\n",
            ">2, 90/468, d1=0.626, d2=0.643 g=0.794\n",
            ">2, 91/468, d1=0.596, d2=0.643 g=0.794\n",
            ">2, 92/468, d1=0.607, d2=0.646 g=0.799\n",
            ">2, 93/468, d1=0.594, d2=0.633 g=0.799\n",
            ">2, 94/468, d1=0.613, d2=0.615 g=0.819\n",
            ">2, 95/468, d1=0.594, d2=0.607 g=0.813\n",
            ">2, 96/468, d1=0.576, d2=0.631 g=0.826\n",
            ">2, 97/468, d1=0.582, d2=0.609 g=0.844\n",
            ">2, 98/468, d1=0.572, d2=0.630 g=0.831\n",
            ">2, 99/468, d1=0.574, d2=0.635 g=0.805\n",
            ">2, 100/468, d1=0.627, d2=0.595 g=0.836\n",
            ">2, 101/468, d1=0.606, d2=0.641 g=0.814\n",
            ">2, 102/468, d1=0.579, d2=0.619 g=0.808\n",
            ">2, 103/468, d1=0.568, d2=0.641 g=0.813\n",
            ">2, 104/468, d1=0.533, d2=0.643 g=0.828\n",
            ">2, 105/468, d1=0.557, d2=0.605 g=0.809\n",
            ">2, 106/468, d1=0.590, d2=0.607 g=0.827\n",
            ">2, 107/468, d1=0.571, d2=0.596 g=0.843\n",
            ">2, 108/468, d1=0.556, d2=0.623 g=0.795\n",
            ">2, 109/468, d1=0.576, d2=0.640 g=0.802\n",
            ">2, 110/468, d1=0.603, d2=0.620 g=0.808\n",
            ">2, 111/468, d1=0.552, d2=0.639 g=0.808\n",
            ">2, 112/468, d1=0.553, d2=0.615 g=0.784\n",
            ">2, 113/468, d1=0.582, d2=0.649 g=0.821\n",
            ">2, 114/468, d1=0.550, d2=0.651 g=0.836\n",
            ">2, 115/468, d1=0.587, d2=0.669 g=0.810\n",
            ">2, 116/468, d1=0.603, d2=0.661 g=0.796\n",
            ">2, 117/468, d1=0.584, d2=0.666 g=0.831\n",
            ">2, 118/468, d1=0.576, d2=0.618 g=0.805\n",
            ">2, 119/468, d1=0.605, d2=0.672 g=0.784\n",
            ">2, 120/468, d1=0.588, d2=0.660 g=0.801\n",
            ">2, 121/468, d1=0.576, d2=0.652 g=0.791\n",
            ">2, 122/468, d1=0.602, d2=0.670 g=0.807\n",
            ">2, 123/468, d1=0.579, d2=0.704 g=0.768\n",
            ">2, 124/468, d1=0.571, d2=0.706 g=0.788\n",
            ">2, 125/468, d1=0.621, d2=0.657 g=0.800\n",
            ">2, 126/468, d1=0.600, d2=0.672 g=0.789\n",
            ">2, 127/468, d1=0.590, d2=0.639 g=0.798\n",
            ">2, 128/468, d1=0.590, d2=0.683 g=0.798\n",
            ">2, 129/468, d1=0.580, d2=0.638 g=0.815\n",
            ">2, 130/468, d1=0.601, d2=0.625 g=0.806\n",
            ">2, 131/468, d1=0.596, d2=0.666 g=0.781\n",
            ">2, 132/468, d1=0.583, d2=0.642 g=0.817\n",
            ">2, 133/468, d1=0.585, d2=0.644 g=0.816\n",
            ">2, 134/468, d1=0.594, d2=0.645 g=0.837\n",
            ">2, 135/468, d1=0.586, d2=0.663 g=0.827\n",
            ">2, 136/468, d1=0.626, d2=0.649 g=0.846\n",
            ">2, 137/468, d1=0.583, d2=0.596 g=0.838\n",
            ">2, 138/468, d1=0.564, d2=0.627 g=0.858\n",
            ">2, 139/468, d1=0.543, d2=0.613 g=0.849\n",
            ">2, 140/468, d1=0.618, d2=0.625 g=0.861\n",
            ">2, 141/468, d1=0.552, d2=0.613 g=0.841\n",
            ">2, 142/468, d1=0.539, d2=0.601 g=0.833\n",
            ">2, 143/468, d1=0.558, d2=0.642 g=0.851\n",
            ">2, 144/468, d1=0.542, d2=0.669 g=0.811\n",
            ">2, 145/468, d1=0.561, d2=0.669 g=0.832\n",
            ">2, 146/468, d1=0.559, d2=0.651 g=0.830\n",
            ">2, 147/468, d1=0.580, d2=0.699 g=0.821\n",
            ">2, 148/468, d1=0.564, d2=0.657 g=0.772\n",
            ">2, 149/468, d1=0.589, d2=0.674 g=0.768\n",
            ">2, 150/468, d1=0.578, d2=0.667 g=0.797\n",
            ">2, 151/468, d1=0.542, d2=0.694 g=0.781\n",
            ">2, 152/468, d1=0.552, d2=0.683 g=0.788\n",
            ">2, 153/468, d1=0.549, d2=0.749 g=0.817\n",
            ">2, 154/468, d1=0.604, d2=0.663 g=0.749\n",
            ">2, 155/468, d1=0.574, d2=0.671 g=0.804\n",
            ">2, 156/468, d1=0.557, d2=0.731 g=0.804\n",
            ">2, 157/468, d1=0.571, d2=0.663 g=0.824\n",
            ">2, 158/468, d1=0.574, d2=0.696 g=0.836\n",
            ">2, 159/468, d1=0.556, d2=0.644 g=0.829\n",
            ">2, 160/468, d1=0.558, d2=0.704 g=0.827\n",
            ">2, 161/468, d1=0.567, d2=0.670 g=0.805\n",
            ">2, 162/468, d1=0.533, d2=0.664 g=0.839\n",
            ">2, 163/468, d1=0.537, d2=0.645 g=0.809\n",
            ">2, 164/468, d1=0.519, d2=0.595 g=0.807\n",
            ">2, 165/468, d1=0.556, d2=0.644 g=0.825\n",
            ">2, 166/468, d1=0.517, d2=0.680 g=0.868\n",
            ">2, 167/468, d1=0.563, d2=0.664 g=0.838\n",
            ">2, 168/468, d1=0.498, d2=0.646 g=0.851\n",
            ">2, 169/468, d1=0.491, d2=0.647 g=0.818\n",
            ">2, 170/468, d1=0.474, d2=0.658 g=0.875\n",
            ">2, 171/468, d1=0.506, d2=0.636 g=0.836\n",
            ">2, 172/468, d1=0.493, d2=0.661 g=0.818\n",
            ">2, 173/468, d1=0.491, d2=0.658 g=0.873\n",
            ">2, 174/468, d1=0.508, d2=0.615 g=0.852\n",
            ">2, 175/468, d1=0.471, d2=0.634 g=0.859\n",
            ">2, 176/468, d1=0.495, d2=0.619 g=0.886\n",
            ">2, 177/468, d1=0.526, d2=0.626 g=0.852\n",
            ">2, 178/468, d1=0.534, d2=0.605 g=0.871\n",
            ">2, 179/468, d1=0.500, d2=0.609 g=0.856\n",
            ">2, 180/468, d1=0.495, d2=0.627 g=0.859\n",
            ">2, 181/468, d1=0.481, d2=0.658 g=0.854\n",
            ">2, 182/468, d1=0.505, d2=0.621 g=0.881\n",
            ">2, 183/468, d1=0.501, d2=0.630 g=0.882\n",
            ">2, 184/468, d1=0.510, d2=0.607 g=0.895\n",
            ">2, 185/468, d1=0.516, d2=0.633 g=0.867\n",
            ">2, 186/468, d1=0.487, d2=0.609 g=0.858\n",
            ">2, 187/468, d1=0.503, d2=0.630 g=0.851\n",
            ">2, 188/468, d1=0.531, d2=0.593 g=0.871\n",
            ">2, 189/468, d1=0.549, d2=0.600 g=0.891\n",
            ">2, 190/468, d1=0.521, d2=0.608 g=0.886\n",
            ">2, 191/468, d1=0.502, d2=0.610 g=0.897\n",
            ">2, 192/468, d1=0.533, d2=0.614 g=0.877\n",
            ">2, 193/468, d1=0.540, d2=0.619 g=0.918\n",
            ">2, 194/468, d1=0.515, d2=0.581 g=0.894\n",
            ">2, 195/468, d1=0.579, d2=0.640 g=0.861\n",
            ">2, 196/468, d1=0.506, d2=0.607 g=0.895\n",
            ">2, 197/468, d1=0.563, d2=0.582 g=0.845\n",
            ">2, 198/468, d1=0.578, d2=0.588 g=0.837\n",
            ">2, 199/468, d1=0.545, d2=0.648 g=0.876\n",
            ">2, 200/468, d1=0.526, d2=0.633 g=0.913\n",
            ">2, 201/468, d1=0.484, d2=0.631 g=0.902\n",
            ">2, 202/468, d1=0.552, d2=0.608 g=0.907\n",
            ">2, 203/468, d1=0.586, d2=0.622 g=0.908\n",
            ">2, 204/468, d1=0.477, d2=0.551 g=0.906\n",
            ">2, 205/468, d1=0.504, d2=0.572 g=0.867\n",
            ">2, 206/468, d1=0.540, d2=0.651 g=0.902\n",
            ">2, 207/468, d1=0.548, d2=0.600 g=0.893\n",
            ">2, 208/468, d1=0.526, d2=0.593 g=0.926\n",
            ">2, 209/468, d1=0.548, d2=0.645 g=0.895\n",
            ">2, 210/468, d1=0.570, d2=0.626 g=0.918\n",
            ">2, 211/468, d1=0.513, d2=0.626 g=0.874\n",
            ">2, 212/468, d1=0.566, d2=0.565 g=0.862\n",
            ">2, 213/468, d1=0.531, d2=0.635 g=0.863\n",
            ">2, 214/468, d1=0.563, d2=0.679 g=0.938\n",
            ">2, 215/468, d1=0.516, d2=0.673 g=0.878\n",
            ">2, 216/468, d1=0.522, d2=0.636 g=0.876\n",
            ">2, 217/468, d1=0.582, d2=0.607 g=0.925\n",
            ">2, 218/468, d1=0.542, d2=0.641 g=0.901\n",
            ">2, 219/468, d1=0.590, d2=0.603 g=0.891\n",
            ">2, 220/468, d1=0.533, d2=0.628 g=0.923\n",
            ">2, 221/468, d1=0.500, d2=0.618 g=0.871\n",
            ">2, 222/468, d1=0.581, d2=0.601 g=0.920\n",
            ">2, 223/468, d1=0.507, d2=0.568 g=0.930\n",
            ">2, 224/468, d1=0.518, d2=0.576 g=0.894\n",
            ">2, 225/468, d1=0.532, d2=0.599 g=0.914\n",
            ">2, 226/468, d1=0.478, d2=0.559 g=0.951\n",
            ">2, 227/468, d1=0.494, d2=0.556 g=0.978\n",
            ">2, 228/468, d1=0.506, d2=0.555 g=0.991\n",
            ">2, 229/468, d1=0.469, d2=0.551 g=1.001\n",
            ">2, 230/468, d1=0.528, d2=0.509 g=0.935\n",
            ">2, 231/468, d1=0.464, d2=0.607 g=0.923\n",
            ">2, 232/468, d1=0.415, d2=0.654 g=0.912\n",
            ">2, 233/468, d1=0.626, d2=0.707 g=0.868\n",
            ">2, 234/468, d1=0.509, d2=0.705 g=0.887\n",
            ">2, 235/468, d1=0.545, d2=0.703 g=0.842\n",
            ">2, 236/468, d1=0.550, d2=0.640 g=0.898\n",
            ">2, 237/468, d1=0.568, d2=0.685 g=0.911\n",
            ">2, 238/468, d1=0.562, d2=0.581 g=0.945\n",
            ">2, 239/468, d1=0.567, d2=0.593 g=0.919\n",
            ">2, 240/468, d1=0.625, d2=0.579 g=0.852\n",
            ">2, 241/468, d1=0.519, d2=0.692 g=0.849\n",
            ">2, 242/468, d1=0.643, d2=0.670 g=0.839\n",
            ">2, 243/468, d1=0.620, d2=0.776 g=0.807\n",
            ">2, 244/468, d1=0.606, d2=0.786 g=0.795\n",
            ">2, 245/468, d1=0.553, d2=0.693 g=0.813\n",
            ">2, 246/468, d1=0.566, d2=0.658 g=0.833\n",
            ">2, 247/468, d1=0.675, d2=0.652 g=0.820\n",
            ">2, 248/468, d1=0.631, d2=0.732 g=0.802\n",
            ">2, 249/468, d1=0.562, d2=0.721 g=0.751\n",
            ">2, 250/468, d1=0.604, d2=0.741 g=0.783\n",
            ">2, 251/468, d1=0.669, d2=0.739 g=0.780\n",
            ">2, 252/468, d1=0.655, d2=0.690 g=0.784\n",
            ">2, 253/468, d1=0.680, d2=0.781 g=0.786\n",
            ">2, 254/468, d1=0.647, d2=0.728 g=0.809\n",
            ">2, 255/468, d1=0.675, d2=0.719 g=0.769\n",
            ">2, 256/468, d1=0.698, d2=0.677 g=0.745\n",
            ">2, 257/468, d1=0.665, d2=0.727 g=0.770\n",
            ">2, 258/468, d1=0.709, d2=0.737 g=0.770\n",
            ">2, 259/468, d1=0.682, d2=0.774 g=0.764\n",
            ">2, 260/468, d1=0.690, d2=0.689 g=0.752\n",
            ">2, 261/468, d1=0.660, d2=0.685 g=0.795\n",
            ">2, 262/468, d1=0.708, d2=0.737 g=0.799\n",
            ">2, 263/468, d1=0.686, d2=0.672 g=0.767\n",
            ">2, 264/468, d1=0.698, d2=0.752 g=0.795\n",
            ">2, 265/468, d1=0.717, d2=0.743 g=0.787\n",
            ">2, 266/468, d1=0.742, d2=0.686 g=0.745\n",
            ">2, 267/468, d1=0.675, d2=0.777 g=0.726\n",
            ">2, 268/468, d1=0.678, d2=0.723 g=0.734\n",
            ">2, 269/468, d1=0.727, d2=0.756 g=0.743\n",
            ">2, 270/468, d1=0.717, d2=0.712 g=0.754\n",
            ">2, 271/468, d1=0.731, d2=0.730 g=0.748\n",
            ">2, 272/468, d1=0.738, d2=0.739 g=0.776\n",
            ">2, 273/468, d1=0.736, d2=0.732 g=0.746\n",
            ">2, 274/468, d1=0.711, d2=0.744 g=0.730\n",
            ">2, 275/468, d1=0.687, d2=0.723 g=0.765\n",
            ">2, 276/468, d1=0.716, d2=0.724 g=0.749\n",
            ">2, 277/468, d1=0.739, d2=0.751 g=0.754\n",
            ">2, 278/468, d1=0.723, d2=0.708 g=0.755\n",
            ">2, 279/468, d1=0.758, d2=0.726 g=0.783\n",
            ">2, 280/468, d1=0.733, d2=0.707 g=0.775\n",
            ">2, 281/468, d1=0.694, d2=0.708 g=0.777\n",
            ">2, 282/468, d1=0.715, d2=0.672 g=0.775\n",
            ">2, 283/468, d1=0.750, d2=0.713 g=0.760\n",
            ">2, 284/468, d1=0.682, d2=0.678 g=0.752\n",
            ">2, 285/468, d1=0.686, d2=0.704 g=0.763\n",
            ">2, 286/468, d1=0.679, d2=0.683 g=0.771\n",
            ">2, 287/468, d1=0.696, d2=0.710 g=0.784\n",
            ">2, 288/468, d1=0.674, d2=0.655 g=0.805\n",
            ">2, 289/468, d1=0.674, d2=0.662 g=0.813\n",
            ">2, 290/468, d1=0.670, d2=0.652 g=0.819\n",
            ">2, 291/468, d1=0.679, d2=0.637 g=0.792\n",
            ">2, 292/468, d1=0.620, d2=0.668 g=0.794\n",
            ">2, 293/468, d1=0.658, d2=0.669 g=0.788\n",
            ">2, 294/468, d1=0.628, d2=0.696 g=0.770\n",
            ">2, 295/468, d1=0.657, d2=0.687 g=0.789\n",
            ">2, 296/468, d1=0.623, d2=0.672 g=0.837\n",
            ">2, 297/468, d1=0.656, d2=0.593 g=0.882\n",
            ">2, 298/468, d1=0.624, d2=0.596 g=0.884\n",
            ">2, 299/468, d1=0.669, d2=0.601 g=0.889\n",
            ">2, 300/468, d1=0.662, d2=0.620 g=0.840\n",
            ">2, 301/468, d1=0.590, d2=0.640 g=0.830\n",
            ">2, 302/468, d1=0.629, d2=0.733 g=0.767\n",
            ">2, 303/468, d1=0.635, d2=0.698 g=0.767\n",
            ">2, 304/468, d1=0.608, d2=0.742 g=0.794\n",
            ">2, 305/468, d1=0.649, d2=0.698 g=0.837\n",
            ">2, 306/468, d1=0.666, d2=0.696 g=0.886\n",
            ">2, 307/468, d1=0.657, d2=0.627 g=0.913\n",
            ">2, 308/468, d1=0.687, d2=0.632 g=0.897\n",
            ">2, 309/468, d1=0.662, d2=0.669 g=0.840\n",
            ">2, 310/468, d1=0.691, d2=0.749 g=0.764\n",
            ">2, 311/468, d1=0.650, d2=0.833 g=0.666\n",
            ">2, 312/468, d1=0.665, d2=0.901 g=0.691\n",
            ">2, 313/468, d1=0.721, d2=0.800 g=0.732\n",
            ">2, 314/468, d1=0.770, d2=0.686 g=0.846\n",
            ">2, 315/468, d1=0.735, d2=0.623 g=0.912\n",
            ">2, 316/468, d1=0.714, d2=0.527 g=1.002\n",
            ">2, 317/468, d1=0.724, d2=0.494 g=1.036\n",
            ">2, 318/468, d1=0.686, d2=0.481 g=1.072\n",
            ">2, 319/468, d1=0.662, d2=0.492 g=1.051\n",
            ">2, 320/468, d1=0.643, d2=0.570 g=0.963\n",
            ">2, 321/468, d1=0.666, d2=0.664 g=0.844\n",
            ">2, 322/468, d1=0.636, d2=0.725 g=0.697\n",
            ">2, 323/468, d1=0.674, d2=0.928 g=0.632\n",
            ">2, 324/468, d1=0.622, d2=0.921 g=0.599\n",
            ">2, 325/468, d1=0.647, d2=0.869 g=0.641\n",
            ">2, 326/468, d1=0.765, d2=0.820 g=0.729\n",
            ">2, 327/468, d1=0.772, d2=0.722 g=0.803\n",
            ">2, 328/468, d1=0.764, d2=0.644 g=0.908\n",
            ">2, 329/468, d1=0.780, d2=0.597 g=0.911\n",
            ">2, 330/468, d1=0.740, d2=0.586 g=0.888\n",
            ">2, 331/468, d1=0.725, d2=0.628 g=0.863\n",
            ">2, 332/468, d1=0.711, d2=0.670 g=0.821\n",
            ">2, 333/468, d1=0.672, d2=0.654 g=0.798\n",
            ">2, 334/468, d1=0.688, d2=0.702 g=0.792\n",
            ">2, 335/468, d1=0.697, d2=0.701 g=0.840\n",
            ">2, 336/468, d1=0.704, d2=0.626 g=0.881\n",
            ">2, 337/468, d1=0.721, d2=0.560 g=0.949\n",
            ">2, 338/468, d1=0.682, d2=0.524 g=1.006\n",
            ">2, 339/468, d1=0.625, d2=0.516 g=1.011\n",
            ">2, 340/468, d1=0.663, d2=0.537 g=0.947\n",
            ">2, 341/468, d1=0.658, d2=0.606 g=0.891\n",
            ">2, 342/468, d1=0.585, d2=0.691 g=0.805\n",
            ">2, 343/468, d1=0.600, d2=0.719 g=0.764\n",
            ">2, 344/468, d1=0.663, d2=0.757 g=0.709\n",
            ">2, 345/468, d1=0.689, d2=0.806 g=0.711\n",
            ">2, 346/468, d1=0.691, d2=0.765 g=0.753\n",
            ">2, 347/468, d1=0.687, d2=0.708 g=0.834\n",
            ">2, 348/468, d1=0.675, d2=0.638 g=0.869\n",
            ">2, 349/468, d1=0.697, d2=0.574 g=0.935\n",
            ">2, 350/468, d1=0.659, d2=0.555 g=0.960\n",
            ">2, 351/468, d1=0.673, d2=0.554 g=0.975\n",
            ">2, 352/468, d1=0.676, d2=0.562 g=0.956\n",
            ">2, 353/468, d1=0.687, d2=0.595 g=0.909\n",
            ">2, 354/468, d1=0.634, d2=0.638 g=0.871\n",
            ">2, 355/468, d1=0.663, d2=0.705 g=0.790\n",
            ">2, 356/468, d1=0.687, d2=0.715 g=0.761\n",
            ">2, 357/468, d1=0.682, d2=0.684 g=0.800\n",
            ">2, 358/468, d1=0.654, d2=0.686 g=0.826\n",
            ">2, 359/468, d1=0.677, d2=0.644 g=0.870\n",
            ">2, 360/468, d1=0.625, d2=0.600 g=0.895\n",
            ">2, 361/468, d1=0.647, d2=0.589 g=0.915\n",
            ">2, 362/468, d1=0.661, d2=0.557 g=0.918\n",
            ">2, 363/468, d1=0.652, d2=0.585 g=0.869\n",
            ">2, 364/468, d1=0.682, d2=0.667 g=0.810\n",
            ">2, 365/468, d1=0.634, d2=0.710 g=0.744\n",
            ">2, 366/468, d1=0.605, d2=0.794 g=0.717\n",
            ">2, 367/468, d1=0.624, d2=0.801 g=0.704\n",
            ">2, 368/468, d1=0.683, d2=0.804 g=0.676\n",
            ">2, 369/468, d1=0.665, d2=0.800 g=0.697\n",
            ">2, 370/468, d1=0.714, d2=0.782 g=0.690\n",
            ">2, 371/468, d1=0.717, d2=0.750 g=0.752\n",
            ">2, 372/468, d1=0.703, d2=0.690 g=0.790\n",
            ">2, 373/468, d1=0.721, d2=0.649 g=0.852\n",
            ">2, 374/468, d1=0.722, d2=0.573 g=0.938\n",
            ">2, 375/468, d1=0.711, d2=0.564 g=0.955\n",
            ">2, 376/468, d1=0.712, d2=0.538 g=0.965\n",
            ">2, 377/468, d1=0.699, d2=0.542 g=0.967\n",
            ">2, 378/468, d1=0.718, d2=0.582 g=0.942\n",
            ">2, 379/468, d1=0.648, d2=0.602 g=0.892\n",
            ">2, 380/468, d1=0.669, d2=0.632 g=0.810\n",
            ">2, 381/468, d1=0.692, d2=0.667 g=0.809\n",
            ">2, 382/468, d1=0.645, d2=0.674 g=0.812\n",
            ">2, 383/468, d1=0.674, d2=0.653 g=0.812\n",
            ">2, 384/468, d1=0.678, d2=0.638 g=0.822\n",
            ">2, 385/468, d1=0.620, d2=0.613 g=0.879\n",
            ">2, 386/468, d1=0.617, d2=0.586 g=0.914\n",
            ">2, 387/468, d1=0.627, d2=0.567 g=0.920\n",
            ">2, 388/468, d1=0.656, d2=0.591 g=0.917\n",
            ">2, 389/468, d1=0.664, d2=0.580 g=0.903\n",
            ">2, 390/468, d1=0.600, d2=0.614 g=0.856\n",
            ">2, 391/468, d1=0.601, d2=0.628 g=0.819\n",
            ">2, 392/468, d1=0.604, d2=0.712 g=0.795\n",
            ">2, 393/468, d1=0.655, d2=0.726 g=0.740\n",
            ">2, 394/468, d1=0.594, d2=0.787 g=0.693\n",
            ">2, 395/468, d1=0.607, d2=0.815 g=0.719\n",
            ">2, 396/468, d1=0.630, d2=0.799 g=0.729\n",
            ">2, 397/468, d1=0.632, d2=0.744 g=0.788\n",
            ">2, 398/468, d1=0.692, d2=0.682 g=0.808\n",
            ">2, 399/468, d1=0.736, d2=0.682 g=0.857\n",
            ">2, 400/468, d1=0.714, d2=0.588 g=0.910\n",
            ">2, 401/468, d1=0.707, d2=0.566 g=0.943\n",
            ">2, 402/468, d1=0.709, d2=0.557 g=0.937\n",
            ">2, 403/468, d1=0.665, d2=0.600 g=0.899\n",
            ">2, 404/468, d1=0.660, d2=0.589 g=0.855\n",
            ">2, 405/468, d1=0.678, d2=0.670 g=0.835\n",
            ">2, 406/468, d1=0.638, d2=0.688 g=0.770\n",
            ">2, 407/468, d1=0.648, d2=0.723 g=0.733\n",
            ">2, 408/468, d1=0.700, d2=0.754 g=0.738\n",
            ">2, 409/468, d1=0.646, d2=0.756 g=0.724\n",
            ">2, 410/468, d1=0.626, d2=0.715 g=0.759\n",
            ">2, 411/468, d1=0.655, d2=0.738 g=0.772\n",
            ">2, 412/468, d1=0.713, d2=0.657 g=0.807\n",
            ">2, 413/468, d1=0.677, d2=0.648 g=0.861\n",
            ">2, 414/468, d1=0.660, d2=0.580 g=0.935\n",
            ">2, 415/468, d1=0.690, d2=0.559 g=0.927\n",
            ">2, 416/468, d1=0.632, d2=0.528 g=0.988\n",
            ">2, 417/468, d1=0.667, d2=0.531 g=1.028\n",
            ">2, 418/468, d1=0.562, d2=0.515 g=0.978\n",
            ">2, 419/468, d1=0.623, d2=0.531 g=0.962\n",
            ">2, 420/468, d1=0.620, d2=0.568 g=0.920\n",
            ">2, 421/468, d1=0.565, d2=0.617 g=0.868\n",
            ">2, 422/468, d1=0.653, d2=0.633 g=0.809\n",
            ">2, 423/468, d1=0.601, d2=0.705 g=0.771\n",
            ">2, 424/468, d1=0.628, d2=0.730 g=0.737\n",
            ">2, 425/468, d1=0.602, d2=0.750 g=0.721\n",
            ">2, 426/468, d1=0.569, d2=0.697 g=0.790\n",
            ">2, 427/468, d1=0.633, d2=0.689 g=0.809\n",
            ">2, 428/468, d1=0.634, d2=0.642 g=0.838\n",
            ">2, 429/468, d1=0.628, d2=0.632 g=0.832\n",
            ">2, 430/468, d1=0.635, d2=0.645 g=0.875\n",
            ">2, 431/468, d1=0.619, d2=0.620 g=0.891\n",
            ">2, 432/468, d1=0.642, d2=0.596 g=0.870\n",
            ">2, 433/468, d1=0.634, d2=0.609 g=0.865\n",
            ">2, 434/468, d1=0.609, d2=0.632 g=0.832\n",
            ">2, 435/468, d1=0.611, d2=0.664 g=0.818\n",
            ">2, 436/468, d1=0.607, d2=0.672 g=0.778\n",
            ">2, 437/468, d1=0.644, d2=0.706 g=0.749\n",
            ">2, 438/468, d1=0.633, d2=0.748 g=0.710\n",
            ">2, 439/468, d1=0.629, d2=0.764 g=0.713\n",
            ">2, 440/468, d1=0.653, d2=0.775 g=0.690\n",
            ">2, 441/468, d1=0.651, d2=0.760 g=0.718\n",
            ">2, 442/468, d1=0.653, d2=0.724 g=0.717\n",
            ">2, 443/468, d1=0.654, d2=0.698 g=0.755\n",
            ">2, 444/468, d1=0.637, d2=0.682 g=0.780\n",
            ">2, 445/468, d1=0.678, d2=0.652 g=0.822\n",
            ">2, 446/468, d1=0.654, d2=0.618 g=0.816\n",
            ">2, 447/468, d1=0.674, d2=0.642 g=0.868\n",
            ">2, 448/468, d1=0.631, d2=0.602 g=0.850\n",
            ">2, 449/468, d1=0.634, d2=0.591 g=0.861\n",
            ">2, 450/468, d1=0.599, d2=0.601 g=0.857\n",
            ">2, 452/468, d1=0.567, d2=0.655 g=0.808\n",
            ">2, 453/468, d1=0.642, d2=0.642 g=0.801\n",
            ">2, 454/468, d1=0.644, d2=0.691 g=0.784\n",
            ">2, 455/468, d1=0.652, d2=0.717 g=0.775\n",
            ">2, 456/468, d1=0.651, d2=0.724 g=0.765\n",
            ">2, 457/468, d1=0.621, d2=0.703 g=0.746\n",
            ">2, 458/468, d1=0.612, d2=0.723 g=0.821\n",
            ">2, 459/468, d1=0.648, d2=0.722 g=0.821\n",
            ">2, 460/468, d1=0.629, d2=0.639 g=0.832\n",
            ">2, 461/468, d1=0.630, d2=0.639 g=0.908\n",
            ">2, 462/468, d1=0.674, d2=0.615 g=0.897\n",
            ">2, 463/468, d1=0.677, d2=0.602 g=0.936\n",
            ">2, 464/468, d1=0.671, d2=0.590 g=0.901\n",
            ">2, 465/468, d1=0.682, d2=0.593 g=0.880\n",
            ">2, 466/468, d1=0.665, d2=0.620 g=0.843\n",
            ">2, 467/468, d1=0.692, d2=0.668 g=0.806\n",
            ">2, 468/468, d1=0.702, d2=0.677 g=0.785\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from math import floor\n",
        "import numpy\n",
        "from numpy import ones\n",
        "from numpy import expand_dims\n",
        "from numpy import log\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from numpy import exp\n",
        "from numpy import cov\n",
        "from numpy import trace\n",
        "from numpy import iscomplexobj\n",
        "from numpy import asarray\n",
        "from numpy.random import shuffle\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from keras.applications.inception_v3 import preprocess_input\n",
        "from keras.datasets.mnist import load_data\n",
        "from keras.datasets import fashion_mnist\n",
        "from skimage.transform import resize\n",
        "from scipy.linalg import sqrtm"
      ],
      "metadata": {
        "id": "g6hISlu5X7SS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The below function is used in both IS and FID to resize the images to fit the inception model\n",
        "# scale an array of images to a new size \n",
        "def scale_images(images, new_shape):\n",
        "\timages_list = list()\n",
        "\tfor image in images:\n",
        "\t\t# resize with nearest neighbor interpolation\n",
        "\t\tnew_image = resize(image, new_shape, 0)\n",
        "\t\t# store\n",
        "\t\timages_list.append(new_image)\n",
        "\treturn asarray(images_list)"
      ],
      "metadata": {
        "id": "VOS_lsJdRYuk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inception Score Calculation"
      ],
      "metadata": {
        "id": "lvxGtbLkZ0ce"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# assumes images have any shape and pixels in [0,255]\n",
        "def calculate_inception_score(images, n_split=10, eps=1E-16):\n",
        "\n",
        "\t\n",
        "\tmodel = InceptionV3()     # loading inception v3 model\n",
        "\t\n",
        "\tscores = list()       # enumerate splits of images/predictions\n",
        "\tn_part = floor(images.shape[0] / n_split)\n",
        "\tfor i in range(n_split):\n",
        "\t\t\n",
        "\t\tix_start, ix_end = i * n_part, (i+1) * n_part    # retrieve images\n",
        "\t\tsubset = images[ix_start:ix_end]\n",
        "\t\t\n",
        "\t\tsubset = subset.astype('float32')    # converting from uint8 to float32\n",
        "\t\t\n",
        "\t\tsubset = scale_images(subset, (299,299,3)) # scaling images to the required size\n",
        "\t\t\n",
        "\t\tsubset = preprocess_input(subset)    # pre-process images, scale to [-1,1]\n",
        "\t\t# predict p(y|x)\n",
        "\t\tp_yx = model.predict(subset)  # predict p(y|x)\n",
        "\t\t\n",
        "\t\tp_y = expand_dims(p_yx.mean(axis=0), 0)     \t# calculate p(y)\n",
        "\t\t\n",
        "\t\tkl_d = p_yx * (log(p_yx + eps) - log(p_y + eps))  # calculate KL divergence using log probabilities\n",
        "\t\t# sum over classes\n",
        "\t\tsum_kl_d = kl_d.sum(axis=1) # sum over classes\n",
        "\t\t\n",
        "\t\tavg_kl_d = mean(sum_kl_d)    # average over images\n",
        "\t\t\n",
        "\t\tis_score = exp(avg_kl_d)   #undo the log\n",
        "\t\t\n",
        "\t\tscores.append(is_score)   \t# store\n",
        "\t# average across images\n",
        "\tis_avg, is_std = mean(scores), std(scores)     # average over images\n",
        "\treturn is_avg, is_std\n",
        " \n",
        "(images, _), (_, _) = fashion_mnist.load_data()\n",
        "images = images[:100]\n",
        "\n",
        "print('loaded', images.shape)\n",
        "# calculation of the inception score\n",
        "is_avg, is_std = calculate_inception_score(images)\n",
        "print('score', is_avg, is_std)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E6DJfJhSRY7a",
        "outputId": "e504274c-67a6-4fde-a6db-746ab566a124"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loaded (100, 28, 28)\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels.h5\n",
            "96116736/96112376 [==============================] - 0s 0us/step\n",
            "96124928/96112376 [==============================] - 0s 0us/step\n",
            "score 3.1019857 0.52531177\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "FID Calculation"
      ],
      "metadata": {
        "id": "GV3-FxkmaFXz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def calculate_fid(model, images1, images2):\n",
        "\n",
        "\t# calculate activations\n",
        "\tact1 = model.predict(images1)\n",
        "\tact2 = model.predict(images2)\n",
        " \n",
        "\t# calculation of mean and covariance statistics\n",
        "\tmu1, sigma1 = act1.mean(axis=0), cov(act1, rowvar=False)\n",
        "\tmu2, sigma2 = act2.mean(axis=0), cov(act2, rowvar=False)\n",
        " \n",
        "\t# calculation of sum squared difference between means\n",
        "\tssdiff = numpy.sum((mu1 - mu2)**2.0)\n",
        " \n",
        "\t# calculate sqrt of product between cov\n",
        "\tcovmean = sqrtm(sigma1.dot(sigma2))\n",
        " \n",
        "\t# check and correct imaginary numbers from sqrt\n",
        "\tif iscomplexobj(covmean):\n",
        "\t\tcovmean = covmean.real\n",
        "\n",
        "\t# calculate score\n",
        "\tfid = ssdiff + trace(sigma1 + sigma2 - 2.0 * covmean)\n",
        "\treturn fid\n",
        "\n",
        "# preparation of the inception v3 model\n",
        "model = InceptionV3(include_top=False, pooling='avg', input_shape=(299,299,3))\n",
        "\n",
        "(images1, _), (images2, _) = fashion_mnist.load_data()\n",
        "\n",
        "images1 = images1[:150]\n",
        "images2 = images2[:150]\n",
        "\n",
        "print('Loaded', images1.shape, images2.shape)\n",
        "\n",
        "# convert integer to floating point values\n",
        "images1 = images1.astype('float32')\n",
        "images2 = images2.astype('float32')\n",
        "\n",
        "# resize images\n",
        "images1 = scale_images(images1, (299,299,3))\n",
        "images2 = scale_images(images2, (299,299,3))\n",
        "print('Scaled', images1.shape, images2.shape)\n",
        "\n",
        "# preprocessing images\n",
        "images1 = preprocess_input(images1)\n",
        "images2 = preprocess_input(images2)\n",
        "\n",
        "# FID Calculation\n",
        "fid = calculate_fid(model, images1, images2)\n",
        "print('FID: %.3f' % fid)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xSpNvHXERpCw",
        "outputId": "c42887df-4ed1-4080-9e45-3879ec7ea74f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded (150, 28, 28) (150, 28, 28)\n",
            "Scaled (150, 299, 299, 3) (150, 299, 299, 3)\n",
            "FID: 81.764\n"
          ]
        }
      ]
    }
  ]
}